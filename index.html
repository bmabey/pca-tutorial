<!DOCTYPE html>
<html>
<head>
  <title>PCA for the uninitiated</title>
  <meta charset="utf-8">
  <meta name="description" content="PCA for the uninitiated">
  <meta name="author" content="Ben Mabey">
  <meta name="generator" content="slidify" />
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta http-equiv="X-UA-Compatible" content="chrome=1">
  <link rel="stylesheet" href="libraries/frameworks/io2012/css/default.css" media="all" >
  <link rel="stylesheet" href="libraries/frameworks/io2012/phone.css" 
    media="only screen and (max-device-width: 480px)" >
  <link rel="stylesheet" href="libraries/frameworks/io2012/css/slidify.css" >
  <link rel="stylesheet" href="libraries/highlighters/highlight.js/css/tomorrow.css" />
  <base target="_blank"> <!-- This amazingness opens all links in a new tab. -->
  <script data-main="libraries/frameworks/io2012/js/slides" 
    src="libraries/frameworks/io2012/js/require-1.0.8.min.js">
  </script>
  
    <link rel="stylesheet" href = "assets/css/foo.css">
<link rel="stylesheet" href = "assets/css/ribbons.css">
<link rel="stylesheet" href = "assets/css/speech_bubbles.css">

</head>
<body style="opacity: 0">
  <slides class="layout-widescreen">

    <!-- TITLE SLIDE -->
    <!-- Should I move this to a Local Layout File? -->
    <slide class="title-slide segue nobackground">
      <aside class="gdbar">
        <img src="assets/img/logos/03_Keyart.png">
      </aside>
      <hgroup class="auto-fadein">
        <h1>PCA for the uninitiated</h1>
        <h2>Intuitive motivation via maximum variance interpretation</h2>
        <p>Ben Mabey
          <br/><a href="http://benmabey.com">benmabey.com</a>
          <br/><a href="http://github.com/bmabey">github.com/bmabey</a>
          <br/><a href="http://twitter.com/bmabey">@bmabey</a>
        </p>
      </hgroup>
      <a href="https://github.com/bmabey/pca-tutorial/zipball/gh-pages" class="example">
        Download
      </a>
          </slide>

    <!-- SLIDES -->
      <slide class="" id="slide-1" style="background:;">
  <hgroup>
    <h2>For offline viewers...</h2>
  </hgroup>
  <article>
    <ul>
<li>Pressing &#39;p&#39; toggles speaker notes (when available)</li>
<li>Pressing &#39;f&#39; toggles fullscreen viewing</li>
<li>Pressing &#39;w&#39; toggles widescreen</li>
<li>Pressing &#39;o&#39; toggles overview mode</li>
</ul>

<p>N.B.: I skip steps and don&#39;t explain eveything on the slides (or even on the presenter notes).
For a more thorough explaination of the math I highly reccomend the tutorial by Jonathan Shlens. The link for that and
more are at the end of this presentation.</p>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-2" style="background:;">
  <hgroup>
    <h2>The ubiquitous &amp; versatile PCA</h2>
  </hgroup>
  <article>
    
<div style='float:left;width:48%;' class='centered'>
  <ul>
<li>Dimensionality Reduction

<ul>
<li>Data Visualization</li>
<li>Learn faster</li>
<li>Lossy Data Compression</li>
</ul></li>
<li>Noise Reduction</li>
<li>Exploration</li>
<li>Feature Extraction</li>
<li>Regression (Orthogonal)</li>
</ul>


</div>
<div style='float:right;width:48%;'>
  <ul>
<li>Unsupervised Learning Algorithm

<ul>
<li>Anomaly Detection (not the best)</li>
<li>Matching/Distance (e.g. <a href="http://en.wikipedia.org/wiki/Eigenface">Eigenfaces</a>, <a href="http://en.wikipedia.org/wiki/Latent_semantic_indexing">LSI</a>)</li>
</ul></li>
<li>K-Means</li>
<li>Computer Graphics (e.g. Bounded Volumes)</li>
<li>and many more across various domains...</li>
</ul>

</div>
  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-3" style="background:;">
  <hgroup>
    <h2>Majority of PCA tutorials...</h2>
  </hgroup>
  <article>
    <ol>
<li>Organize dataset as matrix.</li>
<li>Subtract off the mean for each measurement.</li>
<li>Calculate the covariance matrix and perform eigendecomposition.</li>
<li>Profit!</li>
</ol>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-4" style="background:;">
  <hgroup>
    <h2>Majority of PCA tutorials...</h2>
  </hgroup>
  <article>
    <ol>
<li>Organize dataset as matrix.</li>
<li>Subtract off the mean for each measurement.</li>
<li>Calculate the <del>covariance</del> correlation matrix and perform eigendecomposition.</li>
<li>Profit!</li>
</ol>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-5" style="background:;">
  <hgroup>
    <h2>Majority of PCA tutorials...</h2>
  </hgroup>
  <article>
    <ol>
<li>Organize dataset as matrix.</li>
<li>Subtract off the mean for each measurement.</li>
<li><del>Calculate the <del>covariance</del> correlation matrix and perform eigendecomposition.</del></li>
<li>Perform SVD.</li>
<li>Profit!</li>
</ol>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-6">
<hgroup>
  
</hgroup>
<article class = 'flexbox vcenter'>
<p><img style="height:600px;" src="assets/img/confused_gandalf.jpg"/></p>

</article>
<!-- Presenter Notes -->
</slide>
      <slide class="" id="slide-7" style="background:;">
  <hgroup>
    <h2>The intuitive <del>Magic</del> Math behind PCA</h2>
  </hgroup>
  <article>
    <ul>
<li><strong>Maximize the variance.</strong></li>
<li>Minimize the projection error.</li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-8">
<hgroup>
  
</hgroup>
<article class = 'flexbox vcenter'>
<p><span style="font-size:500%">\(P_{m\times m}X_{m\times n} = Y_{m\times n}\)</span></p>


</article>
<!-- Presenter Notes -->
<aside class="note">
  <section>
    <ul>
<li>N.B.: The X here is the tranpose of a typical design matrix..

<ul>
<li>See the Shlens paper for more info.</li>
</ul></li>
<li>Its goal is to extract the important information from the data and to express this information as a set of new orthogonal variables called principal components.</li>
<li>A <em>linear</em> transformation!  This is a big assumption.</li>
<li><em>Is there another basis, which is a linear combination of the original basis, that best re-expresses our data set?</em></li>
<li>This transformation will become the <em>principal components</em> of X.</li>
<li>What does the transformation boil down to?

<ul>
<li>Rotation and scale.. so how does that help us?</li>
<li>What should our P be doing?</li>
<li>What do we want our Y do look like?</li>
</ul></li>
</ul>

  </section>
</aside>
</slide>
      <slide class="fill nobackground" style="background-image: url(assets/img/signal_noise.png)">
  <hgroup>
    <h2 class="white"></h2>
  </hgroup>
  
  <footer class="source white">http://www.squidoo.com/noise-sources-signal-noise-ratio-snr-and-a-look-at-them-in-the-frequency-domain</footer>
  <!-- Presenter Notes -->
  <aside class="note">
    <section>
      <ul>
<li>Every dataset has noise and signal... How can we bring out the signal?</li>
</ul>

    </section>
  </aside>
</slide>
      <slide class="" id="slide-10">
<hgroup>
  
</hgroup>
<article class = 'flexbox vcenter'>
<p><span style="font-size:500%">\(SNR = \frac{\sigma^2_{signal}}{\sigma^2_{noise}}\)</span></p>

</article>
<!-- Presenter Notes -->
</slide>
      <slide class="fill nobackground" style="background-image: url(assets/img/svn.png)">
  <hgroup>
    <h2 class=""></h2>
  </hgroup>
  
  <footer class="source "></footer>
</slide>
      <slide class="" id="slide-12" style="background:;">
  <hgroup>
    <h2>Rotate to maximize variance</h2>
  </hgroup>
  <article>
    
<div style='float:left;width:48%;' class='centered'>
  <p><img style="width:500px" src="assets/img/svn.png"/></p>


</div>
<div style='float:right;width:48%;'>
  <p><img src="assets/img/snr_rotation.png"/></p>

</div>
  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-13">
<hgroup>
  
</hgroup>
<article class = 'flexbox vcenter'>
<p><img style="height:600px" src="assets/img/redundancy.jpg"/></p>


</article>
<!-- Presenter Notes -->
<aside class="note">
  <section>
    <ul>
<li>The logo dimension and the text dimension are redunant and we can reduce it down to a single dimension. :)</li>
<li>&quot;In the real world&quot; you are given datasets with redundanat data all the time...

<ul>
<li>Different kinds of measurements of same event (i.e. different types of brain scans)</li>
<li>Even duplicated features with transform + noise.</li>
</ul></li>
<li>We want a set of features, principal components, that are not redundant.  That way we can select the most &quot;principal&quot; ones and throw away the rest.</li>
<li>Another name for redundancy is correlation.

<ul>
<li>We want to decorrelate the variables.</li>
</ul></li>
</ul>

  </section>
</aside>
</slide>
      <slide class="" id="slide-14" style="background:;">
  <hgroup>
    
  </hgroup>
  <article>
    <pre><code class="r">library(PerformanceAnalytics)
chart.Correlation(iris[-5], bg = iris$Species, pch = 21)
</code></pre>

<p><img src="figure/pairs.png" alt="plot of chunk pairs"> </p>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-15" style="background:;">
  <hgroup>
    
  </hgroup>
  <article>
    <pre><code class="r">chart.Correlation(decorrelated.iris, bg = iris$Species, pch = 21)
</code></pre>

<p><img src="figure/decorrelated_pairs.png" alt="plot of chunk decorrelated_pairs"> </p>


  </article>
  <!-- Presenter Notes -->
  <aside class="note">
    <section>
      <ul>
<li>Of course PCA doesn&#39;t work by looking at scatter plot pairs.

<ul>
<li>it needs to optimize against the quantified version of this... Covariance/Correlation.</li>
</ul></li>
</ul>

    </section>
  </aside>
</slide>

      <slide class="" id="slide-16">
<hgroup>
  <h2>Variance and Covariance</h2>
</hgroup>
<article class = 'flexbox vcenter'>
<p>\(
   \DeclareMathOperator{\stddev}{stddev}
   \DeclareMathOperator{\var}{var}
   \DeclareMathOperator{\cov}{cov}
   \DeclareMathOperator{\corr}{corr}
\)</p>

<div class="build">
<table>
<thead>
<th></th>
<th>Mathematically Useful</th>
<th style="text-align:center">Intuitive</th>
</thead>
<tr>
<td>Dispersion</td>
<td>
\[
\begin{eqnarray*}
\sigma^2_A = \var(A) &=& E[(A - \mu_A)^2] \\
                     &=& \frac{1}{n} \sum_{i=1}^n (a_i - \mu_A)^2
\end{eqnarray*}
\]
</td>
<td>$$\sigma_A = \stddev(A) = \sqrt{\var(A)}$$</td>
</tr>
<tr>
<td>Relationship</td>
<td>
\[
\begin{eqnarray*}
\sigma_{AB} = \cov(A,B) &=& E[(A - \mu_A)(B - \mu_B)] \\
                     &=& \frac{1}{n} \sum_{i=1}^n (a_i - \mu_A)(b_i - \mu_B)
\end{eqnarray*}
\]
</td>
<td>$$\rho_{AB} = \frac{\sigma_{AB}}{\sigma_A\ \sigma_B} = \frac{\cov(AB)}{\stddev(A) \stddev(B)}$$
unitless measure $(-1.0..1.0)$
</td>
</tr>
</table>
<div> $$\cov(A,A) = \var(A)$$ </div>
<div> $\sigma_{AB}$ or $\rho_{AB}$ is $0$ if and only if $A$ and $B$ are uncorrelated.</div>
</div>

</article>
<!-- Presenter Notes -->
</slide>
      <slide class="" id="slide-17" style="background:;">
  <hgroup>
    <h2>Covariance Matrix</h2>
  </hgroup>
  <article>
    <p>\[
\Sigma
= \begin{bmatrix}
 \sigma_{1,1} & \sigma_{1,2} & \cdots &  \sigma_{1,n} \\ \\
 \sigma_{2,1} & \sigma_{2,2} & \cdots &  \sigma_{2,n} \\ \\
 \vdots & \vdots & \ddots & \vdots \\ \\
 \sigma_{n,1} & \sigma_{n,2} & \cdots &  \sigma_{n,n} \\ \\
\end{bmatrix}
\]</p>

<div class="build">
<div>Preprocess $X$ so that it has zero mean.
Now $\sigma_{AB} = \frac 1n \sum_{i=1}^n a_i b_i$</div>
<div style="font-size:300%">$$\Sigma_X = \frac{1}{n}X^TX$$</div>
</div>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-18" style="background:;">
  <hgroup>
    
  </hgroup>
  <article>
    <pre><code class="r">center &lt;- function(x) x - mean(x)
iris.centered &lt;- apply(as.matrix(iris[-5]), 2, center)
(t(iris.centered) %*% iris.centered)/(nrow(iris) - 1)
</code></pre>

<pre><code>##              Sepal.Length Sepal.Width Petal.Length Petal.Width
## Sepal.Length      0.68569    -0.04243       1.2743      0.5163
## Sepal.Width      -0.04243     0.18998      -0.3297     -0.1216
## Petal.Length      1.27432    -0.32966       3.1163      1.2956
## Petal.Width       0.51627    -0.12164       1.2956      0.5810
</code></pre>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-19" style="background:;">
  <hgroup>
    
  </hgroup>
  <article>
    <pre><code class="r">center &lt;- function(x) x - mean(x)
m.centered &lt;- apply(as.matrix(iris[-5]), 2, center)
(t(m.centered) %*% m.centered)/(nrow(iris) - 1)
</code></pre>

<pre><code>##              Sepal.Length Sepal.Width Petal.Length Petal.Width
## Sepal.Length      0.68569    -0.04243       1.2743      0.5163
## Sepal.Width      -0.04243     0.18998      -0.3297     -0.1216
## Petal.Length      1.27432    -0.32966       3.1163      1.2956
## Petal.Width       0.51627    -0.12164       1.2956      0.5810
</code></pre>

<pre><code class="r">cov(iris[-5])
</code></pre>

<pre><code>##              Sepal.Length Sepal.Width Petal.Length Petal.Width
## Sepal.Length      0.68569    -0.04243       1.2743      0.5163
## Sepal.Width      -0.04243     0.18998      -0.3297     -0.1216
## Petal.Length      1.27432    -0.32966       3.1163      1.2956
## Petal.Width       0.51627    -0.12164       1.2956      0.5810
</code></pre>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-20" style="background:;">
  <hgroup>
    <h2>What would our ideal \(\Sigma_Y\) look like?</h2>
  </hgroup>
  <article>
    <p><span style="font-size:200%">
\[PX = Y\]
</span></p>

<div class="build">
<div>
\[
\Sigma_Y =
 \begin{bmatrix}
   \sigma^2_1\\
    & \sigma^2_2 & \Huge 0\\
    & & \ddots\\
    & \Huge 0 & & \sigma^2_n\\
 \end{bmatrix}
\]
i.e. $Y$ is decorrelated.
</div>

<p></div></p>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-21">
<hgroup>
  <h2>Our goal...</h2>
</hgroup>
<article class = 'flexbox vcenter'>
<p><span style="font-size:250%">
Find some orthonormal matrix \(P\) in \(PX = Y\) such that \(\Sigma_Y = YY^T\) is a diagonal matrix. The rows \(Y_n\)
of \(P\) are the <strong>principal components</strong> of \(X\).
</span></p>

<div class="build">
<span>
Note, that I transposed the design matrix (the data) so that covariance calculation is also reversed.
This will make our life easier...
</span>
</div>

</article>
<!-- Presenter Notes -->
</slide>
      <slide class="" id="slide-22" style="background:;">
  <hgroup>
    <h2>Rewrite \(\Sigma_Y\) in terms of the unknown...</h2>
  </hgroup>
  <article>
    <p><span style="font-size:150%">
\[
\begin{eqnarray*}
\Sigma_Y &=& \frac 1n YY^T \\
         &=& \frac 1n (PX)(PX)^T \\
         &=& \frac 1n PXX^TP^T \\
         &=& P (\frac 1n XX^T) P^T \\
\Sigma_Y &=& P \Sigma_X P^T
\end{eqnarray*}
\]
</span></p>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="fill nobackground" style="background-image: url(assets/img/strang.jpg)">
  <hgroup>
    <h2 class="white">Spectral Theorem / Principal Axis Theorem</h2>
  </hgroup>
  <div style="float:left;width:300px">
</div>

<div style="width:500px; float:right;">
<blockquote class="rectangle-speech-border">
<p>Every symmetric matrix has the eigendecomposition (i.e. can be diagnolized) of:
\[A = Q\Lambda Q^{-1} = Q\Lambda Q^T \]</p>
</blockquote>
</div>

  <footer class="source white"></footer>
</slide>
      <slide class="" id="slide-24" style="background:;">
  <hgroup>
    <h2>Remember, we are choosing what \(P\) is...</h2>
  </hgroup>
  <article>
    <p><span style="font-size:200%">
\[PX = Y\]
</span></p>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-25" style="background:;">
  <hgroup>
    <h2>Remember, we are choosing what \(P\) is...</h2>
  </hgroup>
  <article>
    <p><span style="font-size:175%">Let every row, \(p_i\), be an eigenvector of \(\Sigma_X\).
What this means is that \[P=Q^T\] where \(Q\) comes from the eigendecomposition of \(\Sigma_X\).\[\Sigma_X = Q\Lambda Q^T\]</span></p>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-26" style="background:;">
  <hgroup>
    <h2>Turn the Algebra crank...</h2>
  </hgroup>
  <article>
    <p><span style="font-size:150%">
\[
\begin{eqnarray*}
\Sigma_Y &=& P \Sigma_X P^T \\
         &=& P(Q\Lambda Q^T) P^T \\
         &=& P(P^T\Lambda P) P^T \\
         &=& (PP^T)\Lambda (PP^T) \\
         &=& I\Lambda I \\
\Sigma_Y &=& \Lambda_{\Sigma_X} \\
\end{eqnarray*}
\]
</span></p>

<ul class="build fade">
 <li>The principal components are linear combinations of original features of $X$.</li>
 <li>The principal components of $X$ are the eigenvectors of $\Sigma_X$.</li>
 <li>The corresponding eigenvaules lie in $\Sigma_Y$ and represent the variance.</li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-27" style="background:;">
  <hgroup>
    <h2>Manual PCA in R</h2>
  </hgroup>
  <article>
    <pre><code class="r">iris.eigen = eigen(cov(iris.centered))
rownames(iris.eigen$vectors) = colnames(iris.centered)
colnames(iris.eigen$vectors) = c(&quot;PC1&quot;, &quot;PC2&quot;, &quot;PC3&quot;, &quot;PC4&quot;)
iris.eigen
</code></pre>

<pre><code>## $values
## [1] 4.22824 0.24267 0.07821 0.02384
## 
## $vectors
##                   PC1      PC2      PC3     PC4
## Sepal.Length  0.36139 -0.65659 -0.58203  0.3155
## Sepal.Width  -0.08452 -0.73016  0.59791 -0.3197
## Petal.Length  0.85667  0.17337  0.07624 -0.4798
## Petal.Width   0.35829  0.07548  0.54583  0.7537
</code></pre>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-28" style="background:;">
  <hgroup>
    <h2>Make the contributions intuitive...</h2>
  </hgroup>
  <article>
    <pre><code class="r">iris.eigen$vectors^2
</code></pre>

<pre><code>##                   PC1      PC2      PC3     PC4
## Sepal.Length 0.130600 0.431109 0.338759 0.09953
## Sepal.Width  0.007144 0.533136 0.357497 0.10222
## Petal.Length 0.733885 0.030058 0.005812 0.23025
## Petal.Width  0.128371 0.005697 0.297932 0.56800
</code></pre>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-29" style="background:;">
  <hgroup>
    
  </hgroup>
  <article>
    <pre><code class="r">squared &lt;- iris.eigen$vectors^2
sorted.squares &lt;- squared[order(squared[, 1]), 1]
dotplot(sorted.squares, main = &quot;Variable Contributions to PC1&quot;, cex = 1.5, col = &quot;red&quot;)
</code></pre>

<p><img src="figure/unnamed-chunk-6.png" alt="plot of chunk unnamed-chunk-6"> </p>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-30">
<hgroup>
  
</hgroup>
<article class = 'flexbox vcenter'>
<pre><code class="r"># library(FactoMineR); iris.pca &lt;- PCA(iris, quali.sup=5)
plot(iris.pca, choix = &quot;var&quot;, title = &quot;Correlation Circle&quot;)
</code></pre>

<p><img src="figure/unnamed-chunk-7.png" alt="plot of chunk unnamed-chunk-7"> </p>

</article>
<!-- Presenter Notes -->
</slide>
      <slide class="" id="slide-31">
<hgroup>
  
</hgroup>
<article class = 'flexbox vcenter'>
<pre><code class="r"># res.pca &lt;- PCA(decathlon, quanti.sup=11:12, quali.sup = 13)
plot(res.pca, choix = &quot;var&quot;, title = &quot;Correlation Circle&quot;)
</code></pre>

<p><img src="figure/unnamed-chunk-8.png" alt="plot of chunk unnamed-chunk-8"> </p>

</article>
<!-- Presenter Notes -->
</slide>
      <slide class="" id="slide-32" style="background:;">
  <hgroup>
    <h2>What does the variance (eigenvaules) tell us?</h2>
  </hgroup>
  <article>
    <pre><code class="r">iris.eigen$values  # The variance for each corresponding PC
</code></pre>

<pre><code>## [1] 4.22824 0.24267 0.07821 0.02384
</code></pre>

<div class="build">
  <img src="assets/img/pca_transform.png"/>
</div>


  </article>
  <!-- Presenter Notes -->
  <aside class="note">
    <section>
      <ul>
<li> We can easily reduce this down to two dimensions.</li>
</ul>

    </section>
  </aside>
</slide>

      <slide class="" id="slide-33">
<hgroup>
  
</hgroup>
<article class = 'flexbox vcenter'>
<pre><code class="r"># library(FactoMineR); iris.pca &lt;- PCA(iris, quali.sup=5)
plot(iris.pca, habillage = 5, col.hab = c(&quot;green&quot;, &quot;blue&quot;, &quot;red&quot;), title = &quot;Dataset projected onto PC1-2 Subspace&quot;)
</code></pre>

<p><img src="figure/unnamed-chunk-10.png" alt="plot of chunk unnamed-chunk-10"> </p>


</article>
<!-- Presenter Notes -->
<aside class="note">
  <section>
    <ul>
<li>Talk about how a new observation would be projected in and then the distance could be measured for classification or other reasons.</li>
</ul>

  </section>
</aside>
</slide>
      <slide class="" id="slide-34" style="background:;">
  <hgroup>
    <h2>How many components should you keep?</h2>
  </hgroup>
  <article>
    <p>Ratio of variance retained (e.g. 99% is common):</p>

<p><span style="font-weight:200%">
\[\frac{\sum_{i=1}^k \sigma_i}{\sum_{i=1}^n \sigma_i}\]
</span></p>

<pre><code class="r">cumsum(iris.eigen$values/sum(iris.eigen$values))
</code></pre>

<pre><code>## [1] 0.9246 0.9777 0.9948 1.0000
</code></pre>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="scree_plot" id="slide-35" style="background:;">
  <hgroup>
    <h2>The Elbow Test</h2>
  </hgroup>
  <article>
    <div class="build">
<img style="float:right;height:450px;padding-top:100px;" src="assets/img/scree.jpg"/>
</div>

<pre><code class="r">iris.prcomp &lt;- prcomp(iris[-5], center = TRUE, scale = FALSE)
screeplot(iris.prcomp, type = &quot;line&quot;, main = &quot;Scree Plot&quot;)
</code></pre>

<p><img src="figure/unnamed-chunk-12.png" alt="plot of chunk unnamed-chunk-12"> </p>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-36" style="background:;">
  <hgroup>
    <h2>Kaiser Criterion</h2>
  </hgroup>
  <article>
    <p>Keep only the components whose eigenvalue is larger than the average eigenvalue.
For a correlation PCA, this rule boils down to the standard advice to &quot;keep only the eigenvalues larger than 1&quot;.</p>

<pre><code class="r">eigen(cor(iris.centered))$values
</code></pre>

<pre><code>## [1] 2.91850 0.91403 0.14676 0.02071
</code></pre>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-37">
<hgroup>
  <h2>Remeber, always...</h2>
</hgroup>
<article class = 'flexbox vcenter'>
<p><span style="font-size:800%;">CROSS<br/></br/>VALIDATE!</span>
<br />
PCA is overused and commonly misused, so always verify it is helping by cross validating.</p>

</article>
<!-- Presenter Notes -->
</slide>
      <slide class="biplot" id="slide-38" style="background:;">
  <hgroup>
    <h2>Lots of other ways to aid interpretation...</h2>
  </hgroup>
  <article>
    <pre><code class="r">iris.prcomp &lt;- prcomp(iris[-5], center = TRUE, scale = FALSE)
biplot(iris.prcomp)
</code></pre>

<p><img src="figure/unnamed-chunk-14.png" alt="plot of chunk unnamed-chunk-14"> </p>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-39">
<hgroup>
  <h2>Learn more...</h2>
</hgroup>
<article class = 'flexbox vcenter'>
<p><a href="http://factominer.free.fr/book/"><img src="assets/img/rbook.jpg"/></a></p>

</article>
<!-- Presenter Notes -->
</slide>
      <slide class="fill nobackground" style="background-image: url(assets/img/danger_bilbo.jpg)">
  <hgroup>
    <h2 class=""></h2>
  </hgroup>
  
  <footer class="source "></footer>
  <!-- Presenter Notes -->
  <aside class="note">
    <section>
      <ul>
<li>Eigenvectors can freely rotate in the eigenspace.</li>
<li>With repeated eigenvalues you can&#39;t rely on the ordering.</li>
<li>Numerical differences in algorithms can effect the vectors.</li>
<li>The eigenspace is what is really important.</li>
</ul>

    </section>
  </aside>
</slide>
      <slide class="" id="slide-41">
<hgroup>
  
</hgroup>
<article class = 'flexbox vcenter'>
<p><img style="height:600px;" src="assets/img/confused_gandalf.jpg"/></p>


</article>
<!-- Presenter Notes -->
<aside class="note">
  <section>
    <ul>
<li>While on the topic of LOTR... We haven&#39;t answered these questions yet. Now that you have understand the underlying intuition of PCA we can look at why using the correlation matrix would be required in some cases.</li>
<li>We&#39;ll motivate this with an example...</li>
</ul>

  </section>
</aside>
</slide>
      <slide class="" id="slide-42" style="background:;">
  <hgroup>
    <h2>How will PCA perform?</h2>
  </hgroup>
  <article>
    <pre><code class="r">scaled.iris &lt;- iris
scaled.iris$Petal.Length &lt;- iris$Petal.Length/1000
scaled.iris$Petal.Width &lt;- iris$Petal.Width/1000
scaled.iris$Sepal.Width &lt;- iris$Sepal.Width * 10
</code></pre>

<div style="visibility:hidden;">

```
## Warning: zero-length arrow is of indeterminate angle and so skipped
```

![plot of chunk scaled_iris](figure/scaled_iris1.png) ![plot of chunk scaled_iris](figure/scaled_iris2.png) 

</div>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-43" style="background:;">
  <hgroup>
    <h2>Scale Matters</h2>
  </hgroup>
  <article>
    
<div style='float:left;width:48%;' class='centered'>
  <p><img src="figure/scaled_iris1.png"/></p>


</div>
<div style='float:right;width:48%;'>
  <p><img src="figure/scaled_iris2.png"/></p>

</div>
  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-44" style="background:;">
  <hgroup>
    <h2>Correlation Matrix - Standardize the data</h2>
  </hgroup>
  <article>
    <pre><code class="r"># (In practice just use the built-in cor function)
standardize &lt;- function(x) {
    centered &lt;- x - mean(x)
    centered/sd(centered)
}
scaled.iris.standardized &lt;- apply(as.matrix(scaled.iris[-5]), 2, standardize)
(t(scaled.iris.standardized) %*% scaled.iris.standardized)/(nrow(iris) - 1)
</code></pre>

<pre><code>##              Sepal.Length Sepal.Width Petal.Length Petal.Width
## Sepal.Length       1.0000     -0.1176       0.8718      0.8179
## Sepal.Width       -0.1176      1.0000      -0.4284     -0.3661
## Petal.Length       0.8718     -0.4284       1.0000      0.9629
## Petal.Width        0.8179     -0.3661       0.9629      1.0000
</code></pre>


  </article>
  <!-- Presenter Notes -->
  <aside class="note">
    <section>
      <ul>
<li>Mention Kernel PCA.. you can apply non-linear transforms to the data before as well.</li>
</ul>

    </section>
  </aside>
</slide>

      <slide class="" id="slide-45" style="background:;">
  <hgroup>
    <h2>Ok, so why SVD? And how is it equivalent?</h2>
  </hgroup>
  <article>
    <p>Short answer on why:</p>

<ul class="build">
<li> SVD is more numerically stable</li>
<li> More efficient <br/>
 Especially when operating on a wide matrix.. you skip the step of calculating the covariance matrix
 </li>
<li>There are a lot of SVD algoritms and implementations to choose from</li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="fill nobackground" style="background-image: url(assets/img/strang.jpg)">
  <hgroup>
    <h2 class="white">&quot;absolutely a high point of linear algebra&quot;</h2>
  </hgroup>
  <div style="float:left;width:300px">
</div>

<div style="width:500px; float:right;">
<blockquote class="rectangle-speech-border">
<p>Every matrix has the singular value decomposition (SVD) of:
\[A = UDV^T \]
</p>
</blockquote>
</div>


  <footer class="source white"></footer>
  <!-- Presenter Notes -->
  <aside class="note">
    <section>
      <ul>
<li>It really is a high point...</li>
<li>If you don&#39;t already know SVD the details are out of scope for this presentation.. go watch Gilbert Strang&#39;s courses!</li>
<li>I just will bring out one property that will aid in connecting it to PCA.</li>
</ul>

    </section>
  </aside>
</slide>
      <slide class="" id="slide-47" style="background:;">
  <hgroup>
    <h2>Hey, \(AA^T\) and \(A^TA\) look familar...</h2>
  </hgroup>
  <article>
    <p>\[
\begin{eqnarray*}
A &=& U DV^T \\
AA^T &=& UDV^T(UDV^T)^T \\
&=& UDV^TVD^T U^T \\
&=& UDD^TU^T \ \ (V^TV = I\ \mbox{since $V$, and $U$, are orthonormal}) \\
AA^T &=& U D^2 U^T \ \ (\mbox{since $D$ is a diagnol matrix}) \\
\end{eqnarray*}
\]
Recall that eigendecomposition for an orthonormal matrix is \(A = Q \Lambda Q^T\).</p>

<p>Therefore \(U\) are the eigenvectors of \(AA^T\) and \(D^2\) are the eigenvalues.</p>

<p>Likewise \(V\) are the eigenvectors of \(A^TA\) and \(D^2\) are the eigenvalues.</p>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-48" style="background:;">
  <hgroup>
    <h2>Turn the crank once more...</h2>
  </hgroup>
  <article>
    <p>Let a new matrix \(Y = \frac{1}{\sqrt{n}}X^T\) where each column of \(Y\) is mean centered.</p>

<p>\[
\begin{eqnarray*}
Y^TY &=& (\frac{1}{\sqrt{n}}X^T)^T(\frac{1}{\sqrt{n}}X^T) \\
     &=& \frac 1n XX^T \\
Y^TY &=& \Sigma_X
\end{eqnarray*}
\]</p>

<p>So, if we run SVD on our \(Y\) then \(V\) will contain the eigenvectors of \(\Sigma_X\)... \(X\)&#39;s principal components!
Our eigenvalues, the variances, will be \(D^2\).</p>


  </article>
  <!-- Presenter Notes -->
  <aside class="note">
    <section>
      <p>You should really read Jon Shlens&#39; Tutorial since this is where I stole this from.</p>

    </section>
  </aside>
</slide>

      <slide class="" id="slide-49" style="background:;">
  <hgroup>
    <h2>Tada!</h2>
  </hgroup>
  <article>
    <pre><code class="r">y &lt;- iris.centered/sqrt(nrow(iris) - 1)
y.svd &lt;- svd(y)
pcs &lt;- y.svd$v
rownames(pcs) = colnames(iris.centered)
colnames(pcs) = c(&quot;PC1&quot;, &quot;PC2&quot;, &quot;PC3&quot;, &quot;PC4&quot;)
pcs
</code></pre>

<pre><code>##                   PC1      PC2      PC3     PC4
## Sepal.Length  0.36139 -0.65659  0.58203  0.3155
## Sepal.Width  -0.08452 -0.73016 -0.59791 -0.3197
## Petal.Length  0.85667  0.17337 -0.07624 -0.4798
## Petal.Width   0.35829  0.07548 -0.54583  0.7537
</code></pre>

<pre><code class="r">y.svd$d^2  # variances
</code></pre>

<pre><code>## [1] 4.22824 0.24267 0.07821 0.02384
</code></pre>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="fill nobackground" style="background-image: url(assets/img/Gandalf_the_White_returns.png)">
  <hgroup>
    <h2 class="">References and Resources</h2>
  </hgroup>
  <ol>
<li>Jon Shlens (versions 2.0 and 3.1), <cite><a href="http://www.snl.salk.edu/%7Eshlens/">Tutorial on Principal Component Analysis</a></cite></li>
<li>H Abdi and L J Williams (2010), <cite><a href="http://www.universityoftexasatdallascomets.com/%7Eherve/abdi-wireCS-PCA2010-inpress.pdf">Principal component analysis</a></cite></li>
<li>Andrew Ng (2009), <cite><a href="http://see.stanford.edu/materials/aimlcs229/cs229-notes10.pdf">cs229 Lecture Notes 10</a></cite></li>
<li>Andrew Ng (2009), <cite><a href="http://see.stanford.edu/see/lecturelist.aspx?coll=348ca38a-3a6d-4052-937d-cb017338d7b1">cs229 Lectures 14 &amp; 15</a></cite></li>
<li>Christopher Bishop (2006), <cite><a href="http://research.microsoft.com/en-us/um/people/cmbishop/prml/">Pattern Recognition and Machine Learning</a></cite>, section 12.1</li>
<li>Steve Pittard (2012), <cite><a href="http://www.youtube.com/user/biorsph">Principal Components Analysis Using R</a></cite></li>
<li>Quick-R, <cite><a href="http://www.statmethods.net/advstats/factor.html">Principal Components and Factor Analysis</a></cite> (good pointers to additional R packages)</li>
<li>C Ding, X He (2004), <cite><a href="http://ranger.uta.edu/%7Echqding/papers/KmeansPCA1.pdf">K-means Clustering via Principal Component Analysis</a></cite></li>
</ol>

  <footer class="source "></footer>
</slide>
    <slide class="backdrop"></slide>
  </slides>

  <!--[if IE]>
    <script
      src="http://ajax.googleapis.com/ajax/libs/chrome-frame/1/CFInstall.min.js">
    </script>
    <script>CFInstall.check({mode: 'overlay'});</script>
  <![endif]-->
</body>
<!-- Grab CDN jQuery, fall back to local if offline -->
<script src="http://ajax.aspnetcdn.com/ajax/jQuery/jquery-1.7.min.js"></script>
<script>window.jQuery || document.write('<script src="libraries/widgets/quiz/js/jquery-1.7.min.js"><\/script>')</script>
<!-- Load Javascripts for Widgets -->
<!-- MathJax: Fall back to local if CDN offline but local image fonts are not supported (saves >100MB) -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [['$','$'], ['\\(','\\)']],
      processEscapes: true
    }
  });
</script>
<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/2.0-latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
<!-- <script src="https://c328740.ssl.cf1.rackcdn.com/mathjax/2.0-latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script> -->
<script>window.MathJax || document.write('<script type="text/x-mathjax-config">MathJax.Hub.Config({"HTML-CSS":{imageFont:null}});<\/script><script src="libraries/widgets/mathjax/MathJax.js?config=TeX-AMS-MML_HTMLorMML"><\/script>')
</script>
<!-- LOAD HIGHLIGHTER JS FILES -->
<script src="libraries/highlighters/highlight.js/highlight.pack.js"></script>
<script>hljs.initHighlightingOnLoad();</script>
<!-- DONE LOADING HIGHLIGHTER JS FILES -->
<script src="http://www.google-analytics.com/urchin.js" type="text/javascript">
</script>
<script type="text/javascript">
_uacct = "UA-1747830-2";
urchinTracker();
</script>
</html>